
---
title: "Diabetes Detection Project"
output: 
  html_notebook: 

---

Mackenzie Veazey and Sribindu Sreepada
DS 4420


```{r}
library(dplyr)
library(caret)
library(scales)
library(ggplot2)
library(keras3)
library(tensorflow)
library(randomForest)

tf$constant("Hello TensorFlow!")
```


# Load the data

```{r}
# Load the dataset
df <- read.csv("../archive/diabetes_012_health_indicators_BRFSS2015.csv")

# Convert all columns to integer
df <- df %>% mutate(across(everything(), as.integer))
print(dim(df))

# Balance the dataset by sampling 4631 from each class in Diabetes_012
set.seed(1)
df_balanced3 <- df %>%
  group_by(Diabetes_012) %>%
  sample_n(4631) %>%
  ungroup()
print(dim(df_balanced3))

# Load the already balanced binary dataset and downsample it
df_balanced2 <- read.csv("../archive/diabetes_binary_5050split_health_indicators_BRFSS2015.csv")
df_balanced2 <- df_balanced2 %>% sample_frac(0.3, replace = FALSE, set.seed(1))
print(dim(df_balanced2))

# Function to split data into train/test sets with scaling and bias term
getTrainTestData <- function(df, y_label, test_size = 0.25) {
  # Separate features and target
  y <- df[[y_label]]
  X <- df %>% select(-all_of(y_label))
  
  # Min-max scaling
  X_scaled <- as.data.frame(lapply(X, rescale))
  X_scaled <- round(X_scaled, 3)
  
  # Add bias (intercept) column of ones
  X_scaled$bias <- 1
  
  # Create train/test split
  set.seed(1)
  if (y_label == "Diabetes_012")
    train_indices <- createDataPartition(df_balanced3$Diabetes_012, p = (1 - test_size), list = FALSE)
  else 
    train_indices <- createDataPartition(df_balanced2$Diabetes_binary, p = (1 - test_size), list = FALSE)

  X_train <- X_scaled[train_indices, ]
  X_test <- X_scaled[-train_indices, ]
  y_train <- y[train_indices]
  y_test <- y[-train_indices]
  
  return(list(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test))
}

# Split all datasets
split_bal3 <- getTrainTestData(df_balanced3, "Diabetes_012")
split_bal2 <- getTrainTestData(df_balanced2, "Diabetes_binary")
```








# create keras mlps

### First for the balanced multi-class 
```{r}
model_3 <- keras_model_sequential() 
    
model_3 %>% 
  layer_dense(units = 8, input_shape = c(22)) %>% 
  layer_activation('relu') %>% 
  layer_dense(units = 3) %>% 
  layer_activation('softmax')

# Compile the model
model_3 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# Fit the model
history_3 <- model_3 %>% fit(
  x = as.matrix(split_bal3$X_train),
  y = as.matrix(to_categorical(split_bal3$y_train, num_classes = 3)),
  epochs = 5,
  batch_size = 32,
  verbose = 1
)


# predict
pred <- predict(model_3, as.matrix(split_bal3$X_test))
labels <- c("no diabetes", "prediabetes", "diabetes")
prediction_label <- data.frame(max.col(pred)) %>%     
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
unlist()

# metrics
cm_3 <- table(as.matrix(split_bal3$y_test), prediction_label)
print(cm_3)
plot(history_3)
results <- model_3 %>% evaluate(
  as.matrix(split_bal3$X_test), 
  as.matrix(to_categorical(split_bal3$y_test, num_classes = 3)))
results

cm_3_numeric <- as.numeric(cm_3)

recall <- function(tp, fp) {
  return (tp / (tp + fp))
}


cat(sprintf("Recall: %.3f\n", recall(cm_3_numeric[3], cm_3_numeric[6] + cm_3_numeric[9])))
```





### Then we do for the binary class dataset 

```{r}
model_2 <- keras_model_sequential() 
    
model_2 %>% 
  layer_dense(units = 8, input_shape = c(22)) %>% 
  layer_activation('relu') %>% 
  layer_dense(units = 2) %>% 
  layer_activation('softmax')

# Compile the model
model_2 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# Fit the model
history_2 = model_2 %>% fit(
  x = as.matrix(split_bal2$X_train),
  y = as.matrix(to_categorical(split_bal2$y_train, num_classes = 2)),
  epochs = 5,
  batch_size = 32,
  verbose = 1
)


# predict
pred <- predict(model_2, as.matrix(split_bal2$X_test))
labels <- c("no diabetes", "diabetes")
prediction_label <- data.frame(max.col(pred)) %>%     
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
unlist()

# metrics
cm_2 <- table(as.matrix(split_bal2$y_test), prediction_label)
print(cm_2)
plot(history_2)
results <- model_2 %>% evaluate(
  as.matrix(split_bal2$X_test), 
  as.matrix(to_categorical(split_bal2$y_test, num_classes = 2)))
results

cm_2_numeric <- as.numeric(cm_2)
cat(sprintf("Recall: %.3f\n", recall(cm_2_numeric[2], cm_2_numeric[4])))
```





















# Now get the feature importance


### First for the multi-class dataaset
```{r}
# Combine x feats and y
train_df <- data.frame(split_bal3$X_train)
train_df$target <- as.factor(split_bal3$y_train)
test_df <- data.frame(split_bal3$X_test)
test_df$target <- as.factor(split_bal3$y_test)

# Fit
rf_model <- randomForest(target ~ ., data = train_df, ntree = 100, importance = TRUE)

# Predict on test set
y_pred <- predict(rf_model, newdata = test_df)

# confusion matrix
cm_3_rf <- table(Predicted = y_pred, Actual = test_df$target)
cm_numeric = as.numeric(cm_3_rf)
print(cm_3_rf)

# accuracy + recall
cat(sprintf("Accuracy: %.3f", accuracy))
cat(sprintf("Recall: %.3f\n", recall(cm_numeric[3], cm_numeric[6]+cm_numeric[9])))

# feature importance
varImpPlot(rf_model)


importance_vals <- importance(rf_model)
# gini for classification
importance_df <- data.frame(Feature = rownames(importance_vals),
                            Importance = importance_vals[, "MeanDecreaseGini"])
importance_sorted <- importance_df[order(-importance_df$Importance), ]

top_feats_3 = head(importance_sorted, 10)
top_feats_3

ggplot(importance_sorted, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance (RF) Multi-Class", x = "Features", y = "Mean Decrease in Gini") +
  theme_minimal()

```




### Then the binary class dataset
```{r}
# Combine x feats and y
train_df <- data.frame(split_bal2$X_train)
train_df$target <- as.factor(split_bal2$y_train)
test_df <- data.frame(split_bal2$X_test)
test_df$target <- as.factor(split_bal2$y_test)

# Fit
rf_model <- randomForest(target ~ ., data = train_df, ntree = 100, importance = TRUE)

# Predict on test set
y_pred <- predict(rf_model, newdata = test_df)

# confusion matrix
cm_2_rf <- table(Predicted = y_pred, Actual = test_df$target)
cm_numeric = as.numeric(cm_2_rf)
print(cm_2_rf)

# accuracy + recall
accuracy <- sum(diag(cm_2_rf)) / sum(cm_2_rf)
cat(sprintf("Accuracy: %.3f", accuracy))
cat(sprintf("Recall: %.3f\n", recall(cm_numeric[2], cm_numeric[4])))

# feature importance
varImpPlot(rf_model)

importance_vals <- importance(rf_model)
# gini for classification
importance_df <- data.frame(Feature = rownames(importance_vals),
                            Importance = importance_vals[, "MeanDecreaseGini"])
importance_sorted <- importance_df[order(-importance_df$Importance), ]

top_feats_2 = head(importance_sorted, 10)
top_feats_2

ggplot(importance_sorted, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance (RF) Binary Class", x = "Features", y = "Mean Decrease in Gini") +
  theme_minimal()

```




# Now we can try to recreate the MLPs with the feature selection from the RF

### First we start with the multi-class model
```{r}
model_3 <- keras_model_sequential() 
    
model_3 %>% 
  layer_dense(units = 8, input_shape = c(10)) %>% 
  layer_activation('relu') %>% 
  layer_dense(units = 3) %>% 
  layer_activation('softmax')

# compile the model
model_3 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit the model with the top 10 most important features
history_3 <- model_3 %>% fit(
  x = as.matrix(split_bal3$X_train[top_feats_3$Feature]),
  y = as.matrix(to_categorical(split_bal3$y_train, num_classes = 3)),
  epochs = 5,
  batch_size = 32,
  verbose = 1
)


# predict
pred <- predict(model_3, as.matrix(split_bal3$X_test[top_feats_3$Feature]))
labels <- c("no diabetes", "prediabetes", "diabetes")
prediction_label <- data.frame(max.col(pred)) %>%     
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
unlist()

# metrics
cm_3 <- table(as.matrix(split_bal3$y_test), prediction_label)
print(cm_3)
plot(history_3)
results <- model_3 %>% evaluate(
  as.matrix(split_bal3$X_test[top_feats_3$Feature]), 
  as.matrix(to_categorical(split_bal3$y_test, num_classes = 3)))
results

cm_3_numeric <- as.numeric(cm_3)
cat(sprintf("Recall: %.3f\n", recall(cm_3_numeric[3], cm_3_numeric[6] + cm_3_numeric[9])))
```





### Then we start with the binary-class model
```{r}
model_2 <- keras_model_sequential() 
    
model_2 %>% 
  layer_dense(units = 8, input_shape = c(10)) %>% 
  layer_activation('relu') %>% 
  layer_dense(units = 2) %>% 
  layer_activation('softmax')

# compile the model
model_2 %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = c('accuracy')
)

# fit the model with the top 10 most important features
history_2 <- model_2 %>% fit(
  x = as.matrix(split_bal2$X_train[top_feats_2$Feature]),
  y = as.matrix(to_categorical(split_bal2$y_train, num_classes = 2)),
  epochs = 5,
  batch_size = 32,
  verbose = 1
)


# predict
pred <- predict(model_2, as.matrix(split_bal2$X_test[top_feats_2$Feature]))
labels <- c("no diabetes", "diabetes")
prediction_label <- data.frame(max.col(pred)) %>%     
mutate(pred=labels[max.col.pred.]) %>%
select(2) %>%
unlist()

# metrics
cm_2 <- table(as.matrix(split_bal2$y_test), prediction_label)
print(cm_2)
plot(history_2)
results <- model_2 %>% evaluate(
  as.matrix(split_bal2$X_test[top_feats_2$Feature]), 
  as.matrix(to_categorical(split_bal2$y_test, num_classes = 2)))
results

cm_2_numeric <- as.numeric(cm_2)
cat(sprintf("Recall: %.3f\n", recall(cm_2_numeric[2], cm_2_numeric[4])))
```






# some references
https://www.reddit.com/r/rstats/comments/c2iu1q/when_determining_variable_importance_for_random/
https://www.datacamp.com/tutorial/neural-network-models-r
https://cran.r-project.org/web/packages/keras/vignettes/sequential_model.html









